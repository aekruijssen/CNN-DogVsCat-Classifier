{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import imread\n",
    "import cv2\n",
    "import sklearn.utils\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "\n",
    "DATA_PATH = './data/train/'\n",
    "TEST_PERCENT = 0.1\n",
    "SELECT_SUBSET_PERCENT = 0.3\n",
    "\n",
    "# The cat and dog images are of variable size we have to resize them to all the same size.\n",
    "RESIZE_WIDTH=32\n",
    "RESIZE_HEIGHT=32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to load 7500 files\n",
      "Have loaded 1000 samples\n",
      "Have loaded 2000 samples\n",
      "Have loaded 3000 samples\n",
      "Have loaded 4000 samples\n",
      "Have loaded 5000 samples\n",
      "Have loaded 6000 samples\n",
      "Have loaded 7000 samples\n",
      "Train set has dimensionality (6750, 32, 32, 3)\n",
      "Test set has dimensionality (750, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "files = os.listdir(DATA_PATH)\n",
    "# Shuffle so we are selecting about an equal number of dog and cat images.\n",
    "shuffled_files = sklearn.utils.shuffle(files)\n",
    "select_count = int(len(shuffled_files) * SELECT_SUBSET_PERCENT)\n",
    "\n",
    "print('Going to load %i files' % select_count)\n",
    "\n",
    "subset_files_select = shuffled_files[:select_count]\n",
    "\n",
    "DISPLAY_COUNT = 1000\n",
    "\n",
    "for i, input_file in enumerate(subset_files_select):\n",
    "    if i % DISPLAY_COUNT == 0 and i != 0:\n",
    "        print('Have loaded %i samples' % i)\n",
    "        \n",
    "    img = matplotlib.pyplot.imread(DATA_PATH + input_file)\n",
    "    # Resize the images to be the same size.\n",
    "    img = cv2.resize(img, (RESIZE_WIDTH, RESIZE_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
    "    X.append(img)\n",
    "    if 'cat' == input_file.split('.')[0]:\n",
    "        Y.append(0.0)\n",
    "    else:\n",
    "        Y.append(1.0)\n",
    "        \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "test_size = int(len(X) * TEST_PERCENT)\n",
    "\n",
    "test_X = X[:test_size]\n",
    "test_Y = Y[:test_size]\n",
    "train_X = X[test_size:]\n",
    "train_Y = Y[test_size:]\n",
    "\n",
    "print('Train set has dimensionality %s' % str(train_X.shape))\n",
    "print('Test set has dimensionality %s' % str(test_X.shape))\n",
    "\n",
    "# Apply some normalization here.\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X /= 255\n",
    "test_X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Conv2D, MaxPooling2D, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal', input_shape=(RESIZE_WIDTH, RESIZE_HEIGHT, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(512, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='glorot_normal'))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Define loss and objective\n",
    "optimizer = RMSprop(lr=1e-4)\n",
    "loss = 'binary_crossentropy'\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5400 samples, validate on 1350 samples\n",
      "Epoch 1/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5384 - acc: 0.7333 - val_loss: 0.5501 - val_acc: 0.7126\n",
      "Epoch 2/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5343 - acc: 0.7393 - val_loss: 0.5881 - val_acc: 0.6733\n",
      "Epoch 3/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5290 - acc: 0.7367 - val_loss: 0.5365 - val_acc: 0.7304\n",
      "Epoch 4/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5240 - acc: 0.7406 - val_loss: 0.5559 - val_acc: 0.7037\n",
      "Epoch 5/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5196 - acc: 0.7394 - val_loss: 0.5403 - val_acc: 0.7148\n",
      "Epoch 6/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5102 - acc: 0.7539 - val_loss: 0.5376 - val_acc: 0.7237\n",
      "Epoch 7/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5095 - acc: 0.7513 - val_loss: 0.5295 - val_acc: 0.7348\n",
      "Epoch 8/10\n",
      "5400/5400 [==============================] - 18s 3ms/step - loss: 0.5046 - acc: 0.7519 - val_loss: 0.5239 - val_acc: 0.7452\n",
      "Epoch 9/10\n",
      " 600/5400 [==>...........................] - ETA: 14s - loss: 0.4708 - acc: 0.7717"
     ]
    }
   ],
   "source": [
    "batch_size = 150\n",
    "\n",
    "model.fit(train_X, train_Y, batch_size=batch_size, epochs=EPOCHS, validation_split=0.2, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_X, test_Y, batch_size=batch_size, verbose=1)\n",
    "\n",
    "print('')\n",
    "print('Got %.2f%% accuracy' % (acc * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
